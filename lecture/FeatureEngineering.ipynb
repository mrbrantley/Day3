{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Workshop\n",
    "## DSE 200 - Day 3\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this workshop, you will understand:\n",
    "1. **Temporal Feature Engineering**: How to capture cyclic patterns in time-series data\n",
    "2. **Geospatial Feature Engineering**: How to represent location data for prediction tasks\n",
    "3. **Categorical Feature Engineering**: When numbers have meaning beyond their value\n",
    "4. **Text Feature Engineering**: How to convert text into numerical features using TF-IDF\n",
    "\n",
    "### Why Feature Engineering?\n",
    "Linear and polynomial regression have limitations. Feature engineering helps us:\n",
    "- Capture patterns that simple models can't\n",
    "- Reduce overfitting by creating meaningful features\n",
    "- Improve model performance dramatically"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install pandas numpy scikit-learn matplotlib seaborn geopy requests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from geopy.distance import geodesic\n",
    "import requests\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "sns.set_style('whitegrid')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Temporal Feature Engineering\n",
    "### The Challenge: Oscillating Temperature Data\n",
    "\n",
    "**Scenario:** You have hourly temperature data for August. Temperature follows a daily cycle - it rises during the day and falls at night. How can we capture this pattern?\n",
    "\n",
    "**Key Insight:** Linear regression draws a straight line. Polynomial regression can overfit. But temperature is cyclic - what goes up must come down!\n",
    "\n",
    "### Learning Points:\n",
    "- Sin/Cos encoding captures cyclic patterns\n",
    "- Lag features use past values to predict future\n",
    "- Rolling statistics smooth out noise"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_url = 'https://dse200.dev/Day3/hourly_temperatures_august.csv'\n",
    "temp_file = 'hourly_temperatures_august.csv'\n",
    "\n",
    "if not os.path.exists(temp_file):\n",
    "    r = requests.get(data_url)\n",
    "    with open(temp_file, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "df_temp = pd.read_csv(temp_file)\n",
    "df_temp['timestamp'] = pd.to_datetime(df_temp['timestamp'])\n",
    "\n",
    "print(\"Temperature Dataset:\")\n",
    "print(df_temp.head())\n",
    "print(f\"\\nShape: {df_temp.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Visualize the Raw Data\n",
    "Plot temperature vs time to observe the pattern."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Create a line plot of temperature over time\n",
    "# Hint: Use plt.plot(df_temp['timestamp'], df_temp['temperature_f'])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Baseline - Linear Regression\n",
    "Try predicting temperature using only a time index. This should perform poorly!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_temp['time_index'] = range(len(df_temp))\n",
    "\n",
    "split_idx = int(len(df_temp) * 0.7)\n",
    "train_df = df_temp[:split_idx]\n",
    "test_df = df_temp[split_idx:]\n",
    "\n",
    "# TODO: Train a linear regression model\n",
    "# X_train = train_df[['time_index']].values\n",
    "# y_train = train_df['temperature_f'].values\n",
    "# X_test = test_df[['time_index']].values\n",
    "# y_test = test_df['temperature_f'].values\n",
    "\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# TODO: Calculate and print MSE and R²\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# TODO: Plot actual vs predicted\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Polynomial Regression\n",
    "Try using polynomial features. Experiment with different degrees (2, 3, 5, 7)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Create polynomial features and train models with different degrees\n",
    "# Hint: Use PolynomialFeatures from sklearn.preprocessing\n",
    "# degrees = [2, 3, 5, 7]\n",
    "# For each degree:\n",
    "#   - Create polynomial features\n",
    "#   - Train model\n",
    "#   - Calculate MSE and R²\n",
    "#   - Plot results\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Feature Engineering - Cyclic Encoding\n",
    "Now let's engineer better features! Temperature follows a 24-hour cycle. We'll use sin/cos to capture this."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_temp_fe = df_temp.copy()\n",
    "\n",
    "# TODO: Create cyclic features for hour of day\n",
    "# hour_sin = np.sin(2 * np.pi * df_temp_fe['timestamp'].dt.hour / 24)\n",
    "# hour_cos = np.cos(2 * np.pi * df_temp_fe['timestamp'].dt.hour / 24)\n",
    "\n",
    "# TODO: Add these as new columns to df_temp_fe\n",
    "\n",
    "# TODO: Train linear regression with these cyclic features\n",
    "# Use both hour_sin and hour_cos as features\n",
    "\n",
    "# TODO: Calculate and print MSE and R²\n",
    "\n",
    "# TODO: Plot actual vs predicted\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 2: Geospatial Feature Engineering\n",
    "### The Challenge: House Prices and Location\n",
    "\n",
    "**Scenario:** You're predicting house prices in San Diego. Houses close to downtown or UCSD are more expensive. But how do we represent location?\n",
    "\n",
    "**Key Insight:** Latitude/longitude have some linear relationship with price when there's ONE center. But with multiple centers (downtown + UCSD), we need distance features!\n",
    "\n",
    "### Learning Points:\n",
    "- Raw lat/lon might not work well\n",
    "- Distance to important locations is meaningful\n",
    "- Multiple centers require multiple distance features"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_url = 'https://dse200.dev/Day3/real_estate_san_diego.csv'\n",
    "re_file = 'real_estate_san_diego.csv'\n",
    "\n",
    "if not os.path.exists(re_file):\n",
    "    r = requests.get(data_url)\n",
    "    with open(re_file, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "df_re = pd.read_csv(re_file)\n",
    "\n",
    "print(\"Real Estate Dataset:\")\n",
    "print(df_re.head())\n",
    "print(f\"\\nShape: {df_re.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Visualize Geographic Distribution\n",
    "Create a scatter plot of house locations, colored by price."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Create scatter plot with longitude on x-axis, latitude on y-axis\n",
    "# Color points by price\n",
    "# Add markers for San Diego downtown (32.715736, -117.161087) and UCSD (32.8812, -117.2344)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Baseline - Linear Regression with Lat/Lon\n",
    "Try predicting price using just latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "split_idx = int(len(df_re) * 0.7)\n",
    "train_df = df_re[:split_idx]\n",
    "test_df = df_re[split_idx:]\n",
    "\n",
    "# TODO: Train linear regression with latitude and longitude\n",
    "# X_train = train_df[['latitude', 'longitude']].values\n",
    "# y_train = train_df['price'].values\n",
    "\n",
    "# TODO: Calculate and print MSE, RMSE, and R²\n",
    "\n",
    "# TODO: Create scatter plot of actual vs predicted prices\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Feature Engineering - Distance Features\n",
    "Calculate distance from each house to important centers (San Diego downtown and UCSD)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "san_diego_coords = (32.715736, -117.161087)\n",
    "ucsd_coords = (32.8812, -117.2344)\n",
    "\n",
    "# TODO: Calculate distances using geopy.distance.geodesic\n",
    "# For each row in df_re:\n",
    "#   - Calculate distance to San Diego downtown\n",
    "#   - Calculate distance to UCSD\n",
    "#   - Calculate minimum distance to either center\n",
    "\n",
    "# distances_sd = []\n",
    "# distances_ucsd = []\n",
    "# for idx, row in df_re.iterrows():\n",
    "#     dist_sd = geodesic(san_diego_coords, (row['latitude'], row['longitude'])).miles\n",
    "#     dist_ucsd = geodesic(ucsd_coords, (row['latitude'], row['longitude'])).miles\n",
    "#     distances_sd.append(dist_sd)\n",
    "#     distances_ucsd.append(dist_ucsd)\n",
    "\n",
    "# TODO: Add distance columns to df_re\n",
    "# df_re['distance_san_diego'] = distances_sd\n",
    "# df_re['distance_ucsd'] = distances_ucsd\n",
    "# df_re['min_distance'] = np.minimum(distances_sd, distances_ucsd)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Visualize Distance vs Price\n",
    "Plot how price relates to distance from centers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Create scatter plots:\n",
    "# 1. Distance from San Diego vs Price\n",
    "# 2. Distance from UCSD vs Price\n",
    "# 3. Minimum distance vs Price\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Train Model with Distance Features\n",
    "Now train a linear regression using the minimum distance feature."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Split data with distance feature\n",
    "# TODO: Train linear regression with min_distance\n",
    "# TODO: Calculate and print MSE, RMSE, and R²\n",
    "# TODO: Compare with baseline (lat/lon only)\n",
    "# TODO: Plot actual vs predicted\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 3: Categorical Feature Engineering\n",
    "### The Challenge: I-5 Traffic Volume\n",
    "\n",
    "**Scenario:** Predict vehicle volume on I-5 by hour. Is hour 5 really \"1 less\" than hour 6? Or does \"rush hour\" vs \"non-rush hour\" matter more?\n",
    "\n",
    "**Key Insight:** Sometimes numbers have greater meaning than their value. Hour 7, 8, 9, 16, 17, 18 are all \"rush hour\" - they're categorically similar!\n",
    "\n",
    "### Learning Points:\n",
    "- Manual binning based on domain knowledge\n",
    "- Clustering to discover hidden categories\n",
    "- One-hot encoding for categorical features"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_url = 'https://dse200.dev/Day3/i5_traffic_data.csv'\n",
    "traffic_file = 'i5_traffic_data.csv'\n",
    "\n",
    "if not os.path.exists(traffic_file):\n",
    "    r = requests.get(data_url)\n",
    "    with open(traffic_file, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "df_traffic = pd.read_csv(traffic_file)\n",
    "\n",
    "print(\"Traffic Dataset:\")\n",
    "print(df_traffic.head())\n",
    "print(f\"\\nShape: {df_traffic.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Visualize Traffic by Hour\n",
    "Create a scatter plot of vehicle volume by hour."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Create scatter plot of hour vs vehicle_volume\n",
    "# What patterns do you see?\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Baseline - Linear Regression\n",
    "Try predicting volume using just the hour number."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "split_idx = int(len(df_traffic) * 0.7)\n",
    "train_df = df_traffic[:split_idx]\n",
    "test_df = df_traffic[split_idx:]\n",
    "\n",
    "# TODO: Train linear regression with hour\n",
    "# TODO: Calculate and print MSE, RMSE, MAE, and R²\n",
    "# TODO: Plot actual vs predicted\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Manual Binning - Rush Hour Feature\n",
    "Create a binary feature: is_rush_hour (1 if hour is 7-9 or 16-18, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Create rush hour feature\n",
    "# df_traffic['is_rush_hour'] = ((df_traffic['hour'] >= 7) & (df_traffic['hour'] <= 9)) | \\\n",
    "#                               ((df_traffic['hour'] >= 16) & (df_traffic['hour'] <= 18))\n",
    "# df_traffic['is_rush_hour'] = df_traffic['is_rush_hour'].astype(int)\n",
    "\n",
    "# TODO: Visualize average volume by rush hour status\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Train Model with Rush Hour Feature\n",
    "Train linear regression using both hour and is_rush_hour."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Split data\n",
    "# TODO: Train linear regression with ['hour', 'is_rush_hour']\n",
    "# TODO: Calculate and print metrics\n",
    "# TODO: Compare with baseline\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.5: Clustering - Discover Hidden Categories\n",
    "Use K-Means clustering to automatically discover traffic patterns.\n",
    "\n",
    "**Steps:**\n",
    "1. Use Elbow Method to find optimal number of clusters\n",
    "2. Cluster based on hour, volume, and speed\n",
    "3. Create one-hot encoded features for clusters\n",
    "4. Train model with cluster features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Prepare data for clustering\n",
    "# clustering_features = ['hour', 'vehicle_volume', 'avg_speed_mph']\n",
    "# X_cluster = df_traffic[clustering_features].values\n",
    "\n",
    "# TODO: Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "# TODO: Elbow method - try k from 2 to 10\n",
    "# inertias = []\n",
    "# K_range = range(2, 11)\n",
    "# for k in K_range:\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     kmeans.fit(X_scaled)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# TODO: Plot elbow curve\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Choose optimal k (around 4) and fit KMeans\n",
    "# optimal_k = 4\n",
    "# kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "# df_traffic['traffic_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# TODO: Create one-hot encoded features\n",
    "# cluster_dummies = pd.get_dummies(df_traffic['traffic_cluster'], prefix='cluster')\n",
    "# df_traffic_clustered = pd.concat([df_traffic, cluster_dummies], axis=1)\n",
    "\n",
    "# TODO: Visualize clusters\n",
    "# Create scatter plots colored by cluster\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Train model with cluster features\n",
    "# features = ['hour'] + [col for col in df_traffic_clustered.columns if col.startswith('cluster_')]\n",
    "# TODO: Calculate and print metrics\n",
    "# TODO: Compare with baseline and manual binning\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 4: Text Feature Engineering with TF-IDF\n",
    "### The Challenge: Airbnb Listing Prices\n",
    "\n",
    "**Scenario:** Predict Airbnb listing price based on the listing name/description. Text can't be fed directly into models - we need to convert it to numbers!\n",
    "\n",
    "**Key Insight:** TF-IDF (Term Frequency-Inverse Document Frequency) measures how important a word is to a document. Words like \"luxury\" or \"beachfront\" should increase price!\n",
    "\n",
    "### Learning Points:\n",
    "- TF-IDF converts text to numerical features\n",
    "- Rare, descriptive words are more valuable\n",
    "- Text features can be powerful predictors"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_url = 'https://dse200.dev/Day3/airbnb_sd_listings.csv'\n",
    "airbnb_file = 'airbnb_sd_listings.csv'\n",
    "\n",
    "if not os.path.exists(airbnb_file):\n",
    "    r = requests.get(data_url)\n",
    "    with open(airbnb_file, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "df_airbnb = pd.read_csv(airbnb_file)\n",
    "df_airbnb = df_airbnb.dropna(subset=['price', 'name'])\n",
    "df_airbnb = df_airbnb[df_airbnb['price'] > 0]\n",
    "\n",
    "print(\"Airbnb Dataset:\")\n",
    "print(df_airbnb.head())\n",
    "print(f\"\\nShape: {df_airbnb.shape}\")\n",
    "print(\"\\nSample listing names:\")\n",
    "for i, name in enumerate(df_airbnb['name'].head(10), 1):\n",
    "    print(f\"{i}. {name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Explore Text Data\n",
    "Look at the distribution of listing names and prices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Plot price distribution\n",
    "# TODO: Show statistics of name length\n",
    "# df_airbnb['name_length'] = df_airbnb['name'].str.len()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Create TF-IDF Features\n",
    "Convert listing names to TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Initialize TfidfVectorizer\n",
    "# tfidf = TfidfVectorizer(\n",
    "#     max_features=100,      # Keep top 100 words\n",
    "#     stop_words='english',  # Remove common words like 'the', 'a', 'is'\n",
    "#     ngram_range=(1, 2)     # Use single words and pairs\n",
    "# )\n",
    "\n",
    "# TODO: Fit and transform the text\n",
    "# tfidf_features = tfidf.fit_transform(df_airbnb['name'])\n",
    "\n",
    "# TODO: Convert to DataFrame\n",
    "# tfidf_df = pd.DataFrame(\n",
    "#     tfidf_features.toarray(),\n",
    "#     columns=[f'tfidf_{word}' for word in tfidf.get_feature_names_out()]\n",
    "# )\n",
    "\n",
    "# TODO: Combine with original data\n",
    "# df_airbnb_tfidf = pd.concat([df_airbnb.reset_index(drop=True), tfidf_df], axis=1)\n",
    "\n",
    "print(f\"Number of TF-IDF features created: {tfidf_df.shape[1]}\")\n",
    "print(\"\\nTop 20 TF-IDF features:\")\n",
    "print(list(tfidf.get_feature_names_out()[:20]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Analyze Feature Importance\n",
    "Find which words correlate most with price."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Calculate correlation between each TF-IDF feature and price\n",
    "# feature_importance = []\n",
    "# for col in tfidf_df.columns:\n",
    "#     corr = abs(df_airbnb_tfidf[col].corr(df_airbnb_tfidf['price']))\n",
    "#     feature_importance.append((col, corr))\n",
    "\n",
    "# TODO: Sort by correlation\n",
    "# feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# TODO: Print top 15 features\n",
    "# TODO: Visualize as bar chart\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.4: Train Model with TF-IDF Features\n",
    "Use TF-IDF features to predict price."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Split data (use random shuffle for text data!)\n",
    "# df_shuffled = df_airbnb_tfidf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# split_idx = int(len(df_shuffled) * 0.7)\n",
    "\n",
    "# TODO: Get TF-IDF columns\n",
    "# tfidf_cols = [col for col in df_airbnb_tfidf.columns if col.startswith('tfidf_')]\n",
    "\n",
    "# TODO: Train linear regression\n",
    "# TODO: Calculate and print metrics\n",
    "# TODO: Plot actual vs predicted\n",
    "# TODO: Show top words by coefficient magnitude\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
